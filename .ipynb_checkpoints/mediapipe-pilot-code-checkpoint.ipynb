{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediaPipe pilot code\n",
    "\n",
    "The sections below are pilot codes for setting up MediaPipe landmark detection models. The models all use the PC camera as an input source.\n",
    "The code is designed so that all sections are independent. So, to execute a section, simply run the Starter Code section, then the section corresponding to the task to be performed.\n",
    "\n",
    "Features implemented:\n",
    "- Hand static poses classification\n",
    "- Face landmarks detection\n",
    "- Body landmarks detection\n",
    "- Holistic landmarks detection\n",
    "  \n",
    "*Useful links*\n",
    "- [MediaPipe API reference](https://developers.google.com/mediapipe/api/solutions)\n",
    "- [MediaPipe source code](https://github.com/google/mediapipe/blob/master/mediapipe/python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from mediapipe.python.solutions.drawing_utils import DrawingSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions and glocal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "landmarks_style = DrawingSpec(color=(49, 209, 255))\n",
    "connections_style = DrawingSpec(color=(255, 0, 0), thickness=1)\n",
    "\n",
    "# Pose model complexities\n",
    "model_complexities = {'light': 0, 'medium': 1, 'heavy': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand static poses classification\n",
    "\n",
    "This section uses the hand detection model included in MediaPipe.   \n",
    "The detected landmarks are then classified to detect the hand pose among 10 pre-recorded poses.  \n",
    "This code is a good starting point for the classification of static poses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MediaPipe model\n",
    "\n",
    "We must specify the number of hands to be detected, as well as the confidence threshold above which the hand is actually detected.\n",
    "\n",
    "- `mpHands` contains all the MediaPipe models and utility classes ralated to hand detection models\n",
    "- `mpDraw` instantiate a utility class from MediaPipe to directly draw the detected landmarks on the displayed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model used to classify the gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=mp_hand_gesture. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(mp_hand_gesture, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the gesture recognizer model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmp_hand_gesture\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\Louis\\Documents\\Dev\\Python\\main-env\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:193\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy H5 format files (`.h5` extension). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that the legacy SavedModel format is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by `load_model()` in Keras 3. In \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder to reload a TensorFlow SavedModel as an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference-only layer in Keras 3, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.layers.TFSMLayer(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, call_endpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note that your `call_endpoint` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File format not supported: filepath=mp_hand_gesture. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(mp_hand_gesture, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "source": [
    "# Load the gesture recognizer model\n",
    "model = load_model('mp_hand_gesture')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okay',\n",
       " 'peace',\n",
       " 'thumbs up',\n",
       " 'thumbs down',\n",
       " 'call me',\n",
       " 'stop',\n",
       " 'rock',\n",
       " 'live long',\n",
       " 'fist',\n",
       " 'smile']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load class names\n",
    "f = open('gesture.names', 'r')\n",
    "class_names = f.read().split('\\n')\n",
    "f.close()\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main video loop\n",
    "\n",
    "This loop contains the instructions executed for each frame.\n",
    "1. the frame is captured from the camera\n",
    "2. the frame is vertically flipped to be displayed as a mirror\n",
    "3. the frame is converted as an RGB image (instead of BRG)\n",
    "4. we use the MediaPipe hands model to predict the landmarks position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 43\u001b[0m\n\u001b[0;32m     36\u001b[0m mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(\n\u001b[0;32m     37\u001b[0m     image\u001b[38;5;241m=\u001b[39mframe, \n\u001b[0;32m     38\u001b[0m     landmark_list\u001b[38;5;241m=\u001b[39mhands_lms, \n\u001b[0;32m     39\u001b[0m     connections\u001b[38;5;241m=\u001b[39mmpHands\u001b[38;5;241m.\u001b[39mHAND_CONNECTIONS,\n\u001b[0;32m     40\u001b[0m     connection_drawing_spec\u001b[38;5;241m=\u001b[39mmp_drawing_styles\u001b[38;5;241m.\u001b[39mget_default_hand_connections_style())\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Predict gesture (probability distribution)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict([landmarks], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     44\u001b[0m classID \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction)\n\u001b[0;32m     45\u001b[0m class_name \u001b[38;5;241m=\u001b[39m class_names[classID]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read each frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    x, y, c = frame.shape\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # Convert the frame to RGB\n",
    "    frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = hands.process(frame)\n",
    "\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    class_name = ''\n",
    "\n",
    "    # post process the result\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        # Here we only have one hand by default but in general the loops allows to handle several hands\n",
    "        for hands_lms in result.multi_hand_landmarks:\n",
    "            for lm in hands_lms.landmark:\n",
    "                \n",
    "                # Denormalize coordinates\n",
    "                lmx = int(lm.x * x)\n",
    "                lmy = int(lm.y * y)\n",
    "                landmarks.append([lmx, lmy])\n",
    "\n",
    "            # Drawing landmarks on frames\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame, \n",
    "                landmark_list=hands_lms, \n",
    "                connections=mpHands.HAND_CONNECTIONS,\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            # Predict gesture (probability distribution)\n",
    "            prediction = model.predict([landmarks], verbose=0)\n",
    "            classID = np.argmax(prediction)\n",
    "            class_name = class_names[classID]\n",
    "\n",
    "    # show the prediction on the frame\n",
    "    cv2.putText(frame, f'Predicted class: {class_name}', (10, 80), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   1, (49, 209, 255), 2)\n",
    "    \n",
    "    cv2.putText(frame, 'Hand pose estimation', (120, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (49, 209, 255), 2)\n",
    "    cv2.putText(frame, 'Press Q to exit', (120, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,0), 1)\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face landmarks detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MediaPipe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_landmarks = False # SET TO TRUE TO DISPLAY IRISES LANDMARKS\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "faces = mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=refine_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main video loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Landmarks index](https://private-user-images.githubusercontent.com/9028430/322543179-03169849-76e0-4edf-8173-52d576ff612f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc0MDU5NjYsIm5iZiI6MTcxNzQwNTY2NiwicGF0aCI6Ii85MDI4NDMwLzMyMjU0MzE3OS0wMzE2OTg0OS03NmUwLTRlZGYtODE3My01MmQ1NzZmZjYxMmYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDYwMyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA2MDNUMDkwNzQ2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTM2OGJjMmVhNzc5M2M3MDljNzg1NGJhODcwMWM5ZGM2ODE1YWUzNTA3NGFiYjljMTNjMjc3MzdkOWU1ODQyMCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.vqLmL9n6wSXLAlAb99ucAaK31sej2fe5N-2Fqa_KpQE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_EYE_INNER_CORNER = 133\n",
    "LEFT_EYE_OUTER_CORNER = 33\n",
    "RIGHT_EYE_INNER_CORNER = 362\n",
    "RIGHT_EYE_OUTER_CORNER = 263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\Documents\\Dev\\Python\\main-env\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read each frame from the webcam\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    # jump to the next loop if a frame is not captured\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    row, col, _ = frame.shape\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # Convert the frame to RGB\n",
    "    frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = faces.process(frame)\n",
    "\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        for face_lms in result.multi_face_landmarks:\n",
    "            \n",
    "            lms = face_lms.landmark\n",
    "            left_eye_inner_corner = lms[LEFT_EYE_INNER_CORNER]\n",
    "            left_eye_outer_corner = lms[LEFT_EYE_OUTER_CORNER]\n",
    "            right_eye_inner_corner = lms[RIGHT_EYE_INNER_CORNER]\n",
    "            right_eye_outer_corner = lms[RIGHT_EYE_OUTER_CORNER]\n",
    "\n",
    "            # Denormalize\n",
    "            left_eye_inner = (int(left_eye_inner_corner.x * col), int(left_eye_inner_corner.y * row))\n",
    "            left_eye_outer = (int(left_eye_outer_corner.x * col), int(left_eye_outer_corner.y * row))\n",
    "            right_eye_inner = (int(right_eye_inner_corner.x * col), int(right_eye_inner_corner.y * row))\n",
    "            right_eye_outer = (int(right_eye_outer_corner.x * col), int(right_eye_outer_corner.y * row))\n",
    "\n",
    "            cv2.circle(frame, left_eye_inner, 2, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, left_eye_outer, 2, (255, 0, 0), -1)\n",
    "            cv2.circle(frame, right_eye_inner, 2, (0, 0, 255), -1)\n",
    "            cv2.circle(frame, right_eye_outer, 2, (255, 255, 255), -1)\n",
    "\n",
    "            # DRAWING OPTIONS FROM MEDIAPIPE\n",
    "            # Draw face mesh\n",
    "            # mp_drawing.draw_landmarks(\n",
    "            #     image=frame,\n",
    "            #     landmark_list=face_lms,\n",
    "            #     connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            #     landmark_drawing_spec=None,\n",
    "            #     connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "            # Draw face contours\n",
    "            # mp_drawing.draw_landmarks(\n",
    "            #     image=frame,\n",
    "            #     landmark_list=face_lms,\n",
    "            #     connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            #     landmark_drawing_spec=None,\n",
    "            #     connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "\n",
    "            # Draw iris\n",
    "            # mp_drawing.draw_landmarks(\n",
    "            #     image=frame,\n",
    "            #     landmark_list=face_lms,\n",
    "            #     connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "            #     landmark_drawing_spec=None,\n",
    "            #     connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "            \n",
    "\n",
    "\n",
    "    cv2.putText(frame, 'Face landmarks detection', (120, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (49, 209, 255), 2)\n",
    "    cv2.putText(frame, 'Press Q to exit', (120, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,0), 1)\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body landmarks detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to c:\\Users\\Louis\\Documents\\Dev\\Python\\main-env\\lib\\site-packages\\mediapipe/modules/pose_landmark/pose_landmark_lite.tflite\n"
     ]
    }
   ],
   "source": [
    "mp_body_mesh = mp.solutions.pose\n",
    "poses = mp_body_mesh.Pose(smooth_segmentation=True, model_complexity=model_complexities['light'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read each frame from the webcam\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # jump to the next loop if a frame is not captured\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # Convert the frame to RGB\n",
    "    frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = poses.process(frame)\n",
    "\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "    # Draw face mesh\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=frame,\n",
    "        landmark_list=result.pose_landmarks,\n",
    "        connections=mp_body_mesh.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=landmarks_style,\n",
    "        connection_drawing_spec=connections_style\n",
    "        )\n",
    "            \n",
    "\n",
    "\n",
    "    cv2.putText(frame, 'Pose landmarks detection', (120, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (49, 209, 255), 2)\n",
    "    cv2.putText(frame, 'Press Q to exit', (120, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,0), 1)\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holistic model landmarks\n",
    "\n",
    "[holistic.py source code](https://github.com/google/mediapipe/blob/master/mediapipe/python/solutions/holistic.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic_mesh = mp.solutions.holistic\n",
    "holistics = mp_holistic_mesh.Holistic(static_image_mode=False, model_complexity=model_complexities['light'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read each frame from the webcam\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # jump to the next loop if a frame is not captured\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # Convert the frame to RGB\n",
    "    frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = holistics.process(frame)\n",
    "\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "    # Draw face mesh\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=frame,\n",
    "        landmark_list=result.face_landmarks,\n",
    "        connections=mp_holistic_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=connections_style\n",
    "        )\n",
    "            \n",
    "\n",
    "\n",
    "    cv2.putText(frame, 'Holistic landmarks detection', (120, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (49, 209, 255), 2)\n",
    "    cv2.putText(frame, 'Press Q to exit', (120, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,0), 1)\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
