{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediaPipe pilot code\n",
    "\n",
    "The sections below are pilot codes for setting up MediaPipe landmark detection models. The models all use the PC camera as an input source.\n",
    "The code is designed so that all sections are independent. So, to execute a section, simply run the Starter Code section, then the section corresponding to the task to be performed.\n",
    "\n",
    "Features implemented:\n",
    "- Hand static poses classification\n",
    "- Face landmarks detection\n",
    "- Body landmarks detection\n",
    "- Holistic landmarks detection\n",
    "  \n",
    "*Useful links*\n",
    "- [MediaPipe API reference](https://developers.google.com/mediapipe/api/solutions)\n",
    "- [MediaPipe source code](https://github.com/google/mediapipe/blob/master/mediapipe/python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "from mediapipe.python.solutions.drawing_utils import DrawingSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions and glocal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "landmarks_style = DrawingSpec(color=(49, 209, 255))\n",
    "connections_style = DrawingSpec(color=(255, 0, 0), thickness=1)\n",
    "\n",
    "# Pose model complexities\n",
    "model_complexities = {'light': 0, 'medium': 1, 'heavy': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand static poses classification\n",
    "\n",
    "This section uses the hand detection model included in MediaPipe.   \n",
    "The detected landmarks are then classified to detect the hand pose among 10 pre-recorded poses.  \n",
    "This code is a good starting point for the classification of static poses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MediaPipe model\n",
    "\n",
    "We must specify the number of hands to be detected, as well as the confidence threshold above which the hand is actually detected.\n",
    "\n",
    "- `mpHands` contains all the MediaPipe models and utility classes ralated to hand detection models\n",
    "- `mpDraw` instantiate a utility class from MediaPipe to directly draw the detected landmarks on the displayed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model used to classify the gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 42)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                2752      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,362\n",
      "Trainable params: 112,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the gesture recognizer model\n",
    "model = load_model('mp_hand_gesture')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okay',\n",
       " 'peace',\n",
       " 'thumbs up',\n",
       " 'thumbs down',\n",
       " 'call me',\n",
       " 'stop',\n",
       " 'rock',\n",
       " 'live long',\n",
       " 'fist',\n",
       " 'smile']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load class names\n",
    "f = open('gesture.names', 'r')\n",
    "class_names = f.read().split('\\n')\n",
    "f.close()\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main video loop\n",
    "\n",
    "This loop contains the instructions executed for each frame.\n",
    "1. the frame is captured from the camera\n",
    "2. the frame is vertically flipped to be displayed as a mirror\n",
    "3. the frame is converted as an RGB image (instead of BRG)\n",
    "4. we use the MediaPipe hands model to predict the landmarks position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read each frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    x, y, c = frame.shape\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # Convert the frame to RGB\n",
    "    frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = hands.process(frame)\n",
    "\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    class_name = ''\n",
    "\n",
    "    # post process the result\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        # Here we only have one hand by default but in general the loops allows to handle several hands\n",
    "        for hands_lms in result.multi_hand_landmarks:\n",
    "            for lm in hands_lms.landmark:\n",
    "                \n",
    "                # Denormalize coordinates\n",
    "                lmx = int(lm.x * x)\n",
    "                lmy = int(lm.y * y)\n",
    "                landmarks.append([lmx, lmy])\n",
    "\n",
    "            # Drawing landmarks on frames\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame, \n",
    "                landmark_list=hands_lms, \n",
    "                connections=mpHands.HAND_CONNECTIONS,\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            # Predict gesture (probability distribution)\n",
    "            prediction = model.predict([landmarks], verbose=0)\n",
    "            classID = np.argmax(prediction)\n",
    "            class_name = class_names[classID]\n",
    "\n",
    "    # show the prediction on the frame\n",
    "    cv2.putText(frame, f'Predicted class: {class_name}', (10, 80), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   1, (49, 209, 255), 2)\n",
    "    \n",
    "    cv2.putText(frame, 'Hand pose estimation', (120, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (49, 209, 255), 2)\n",
    "    cv2.putText(frame, 'Press Q to exit', (120, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,0), 1)\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face landmarks detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MediaPipe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "faces = mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main video loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read each frame from the webcam\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # jump to the next loop if a frame is not captured\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # Convert the frame to RGB\n",
    "    frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = faces.process(frame)\n",
    "\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        for face_lms in result.multi_face_landmarks:\n",
    "            \n",
    "            # # Draw face mesh\n",
    "            # mp_drawing.draw_landmarks(\n",
    "            #     image=frame,\n",
    "            #     landmark_list=face_lms,\n",
    "            #     connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            #     landmark_drawing_spec=None,\n",
    "            #     connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "            # Draw face contours\n",
    "            # mp_drawing.draw_landmarks(\n",
    "            #     image=frame,\n",
    "            #     landmark_list=face_lms,\n",
    "            #     connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            #     landmark_drawing_spec=None,\n",
    "            #     connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "\n",
    "            # Draw irises\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame,\n",
    "                landmark_list=face_lms,\n",
    "                connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "            \n",
    "\n",
    "\n",
    "    cv2.putText(frame, 'Face landmarks detection', (120, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (49, 209, 255), 2)\n",
    "    cv2.putText(frame, 'Press Q to exit', (120, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,0), 1)\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body landmarks detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_body_mesh = mp.solutions.pose\n",
    "poses = mp_body_mesh.Pose(smooth_segmentation=True, model_complexity=model_complexities['light'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read each frame from the webcam\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # jump to the next loop if a frame is not captured\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # Convert the frame to RGB\n",
    "    frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = poses.process(frame)\n",
    "\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "    # Draw face mesh\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=frame,\n",
    "        landmark_list=result.pose_landmarks,\n",
    "        connections=mp_body_mesh.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=landmarks_style,\n",
    "        connection_drawing_spec=connections_style\n",
    "        )\n",
    "            \n",
    "\n",
    "\n",
    "    cv2.putText(frame, 'Pose landmarks detection', (120, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (49, 209, 255), 2)\n",
    "    cv2.putText(frame, 'Press Q to exit', (120, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,0), 1)\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holistic model landmarks\n",
    "\n",
    "[holistic.py source code](https://github.com/google/mediapipe/blob/master/mediapipe/python/solutions/holistic.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic_mesh = mp.solutions.holistic\n",
    "holistics = mp_holistic_mesh.Holistic(static_image_mode=False, model_complexity=model_complexities['light'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read each frame from the webcam\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # jump to the next loop if a frame is not captured\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # Convert the frame to RGB\n",
    "    frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = holistics.process(frame)\n",
    "\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "    # Draw face mesh\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=frame,\n",
    "        landmark_list=result.face_landmarks,\n",
    "        connections=mp_holistic_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=connections_style\n",
    "        )\n",
    "            \n",
    "\n",
    "\n",
    "    cv2.putText(frame, 'Holistic landmarks detection', (120, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (49, 209, 255), 2)\n",
    "    cv2.putText(frame, 'Press Q to exit', (120, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,0), 1)\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
